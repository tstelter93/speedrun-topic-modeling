{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'archived': True,\n",
      " 'author': 'intheblue',\n",
      " 'author_flair_css_class': '',\n",
      " 'author_flair_text': 'Hotline Miami, Spider Solitare',\n",
      " 'created': 1417395407,\n",
      " 'created_utc': '1417395407',\n",
      " 'distinguished': None,\n",
      " 'domain': 'twitch.tv',\n",
      " 'downs': 0,\n",
      " 'edited': False,\n",
      " 'from': None,\n",
      " 'from_id': None,\n",
      " 'from_kind': None,\n",
      " 'gilded': 0,\n",
      " 'hide_score': False,\n",
      " 'id': '2nw69z',\n",
      " 'is_self': False,\n",
      " 'link_flair_css_class': None,\n",
      " 'link_flair_text': None,\n",
      " 'media': {'oembed': {'description': 'Recorded live on an hour ago',\n",
      "                      'height': 300,\n",
      "                      'html': '&lt;iframe class=\"embedly-embed\" '\n",
      "                              'src=\"//cdn.embedly.com/widgets/media.html?src=http%3A%2F%2Fwww.twitch.tv%2Fwidgets%2Flive_embed_player.swf%3Fchannel%3Ddingodroles&amp;fv=hostname%3Dwww.twitch.tv%26start_volume%3D25%26channel%3Ddingodrole%26auto_play%3Dfalse&amp;url=http%3A%2F%2Fwww.twitch.tv%2Fdingodrole%2Fc%2F5593916&amp;image=http%3A%2F%2Fstatic-cdn.jtvnw.net%2Fjtv_user_pictures%2Fpanel-31452198-image-6e137cc829bea5a9-320.jpeg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=application%2Fx-shockwave-flash&amp;schema=twitch\" '\n",
      "                              'width=\"400\" height=\"300\" scrolling=\"no\" '\n",
      "                              'frameborder=\"0\" '\n",
      "                              'allowfullscreen&gt;&lt;/iframe&gt;',\n",
      "                      'provider_name': 'Twitch.tv',\n",
      "                      'provider_url': 'http://www.twitch.tv',\n",
      "                      'thumbnail_height': 240,\n",
      "                      'thumbnail_url': 'http://static-cdn.jtvnw.net/jtv.thumbs/archive-594375235-320x240.jpg',\n",
      "                      'thumbnail_width': 320,\n",
      "                      'title': 'Hotline Miami any% NG ( 20:16 )',\n",
      "                      'type': 'video',\n",
      "                      'version': '1.0',\n",
      "                      'width': 400},\n",
      "           'type': 'twitch.tv'},\n",
      " 'media_embed': {'content': '&lt;iframe class=\"embedly-embed\" '\n",
      "                            'src=\"//cdn.embedly.com/widgets/media.html?src=http%3A%2F%2Fwww.twitch.tv%2Fwidgets%2Flive_embed_player.swf%3Fchannel%3Ddingodroles&amp;fv=hostname%3Dwww.twitch.tv%26start_volume%3D25%26channel%3Ddingodrole%26auto_play%3Dfalse&amp;url=http%3A%2F%2Fwww.twitch.tv%2Fdingodrole%2Fc%2F5593916&amp;image=http%3A%2F%2Fstatic-cdn.jtvnw.net%2Fjtv_user_pictures%2Fpanel-31452198-image-6e137cc829bea5a9-320.jpeg&amp;key=2aa3c4d5f3de4f5b9120b660ad850dc9&amp;type=application%2Fx-shockwave-flash&amp;schema=twitch\" '\n",
      "                            'width=\"400\" height=\"300\" scrolling=\"no\" '\n",
      "                            'frameborder=\"0\" '\n",
      "                            'allowfullscreen&gt;&lt;/iframe&gt;',\n",
      "                 'height': 300,\n",
      "                 'scrolling': False,\n",
      "                 'width': 400},\n",
      " 'name': 't3_2nw69z',\n",
      " 'num_comments': 6,\n",
      " 'over_18': False,\n",
      " 'permalink': '/r/speedrun/comments/2nw69z/wrhotline_miami_any_ng_in_2016_by_dingodrole/',\n",
      " 'quarantine': False,\n",
      " 'retrieved_on': 1441043841,\n",
      " 'saved': False,\n",
      " 'score': 37,\n",
      " 'secure_media': None,\n",
      " 'secure_media_embed': {},\n",
      " 'selftext': '',\n",
      " 'stickied': False,\n",
      " 'subreddit': 'speedrun',\n",
      " 'subreddit_id': 't5_2sf9e',\n",
      " 'thumbnail': 'http://b.thumbs.redditmedia.com/8eH-7LJF0WhS59rAEiPwqtxlVpWGdTJ_dmliPHSZQXU.jpg',\n",
      " 'title': '[WR]Hotline Miami Any% NG in 20:16 by Dingodrole',\n",
      " 'ups': 37,\n",
      " 'url': 'http://www.twitch.tv/dingodrole/c/5593916'}\n"
     ]
    }
   ],
   "source": [
    "# Import the Data\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "#Read in submissions\n",
    "with open('r_data/RS_2014-12_speedrun.json', 'r', encoding='utf-8') as file:\n",
    "    submission_dataset = json.loads(file.read())\n",
    "\n",
    "# Read in comments\n",
    "with open('r_data/RC_2014-12_speedrun.json', 'r', encoding='utf-8') as file:\n",
    "    comment_dataset = json.loads(file.read())\n",
    "\n",
    "file.close()\n",
    "#pprint(type(submission_dataset))\n",
    "pprint(submission_dataset[0])\n",
    "#pprint(comment_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission 0:  \n",
      "{'id': '2nw69z', 'title': '[WR]Hotline Miami Any% NG in 20:16 by Dingodrole', 'selftext': '', 'score': 37, 'num_comments': 6}\n",
      "\n",
      "\n",
      "Comment 0:  \n",
      "{'parent_id': 't3_2nvw48', 'body': 'this only beats my PB time by around a minute :/ a TAS of this should be pretty close to 48:30 easily...', 'score': 6}\n"
     ]
    }
   ],
   "source": [
    "# Data preporcessing part 1\n",
    "# We curate the tags we want.       \n",
    "\n",
    "# process submission\n",
    "info = {}\n",
    "submission_dataset_two = []\n",
    "for dictionary in submission_dataset:\n",
    "    info = {\n",
    "        \"id\":            dictionary.get('id'),\n",
    "        \"title\":         dictionary.get('title'),\n",
    "        \"selftext\":      dictionary.get('selftext'),\n",
    "        \"score\":         dictionary.get('score'),\n",
    "        \"num_comments\":  dictionary.get('num_comments')\n",
    "    }\n",
    "    submission_dataset_two.append(info)\n",
    "print(\"Submission 0:  \")\n",
    "print(submission_dataset_two[0])\n",
    "print(\"\\n\")\n",
    "\n",
    "info_two = {}\n",
    "comment_dataset_two = []\n",
    "for dictionary in comment_dataset:\n",
    "    info_two = {\n",
    "        \"parent_id\": dictionary.get('parent_id'),\n",
    "        \"body\":      dictionary.get('body'),\n",
    "        \"score\":     dictionary.get('score')\n",
    "    }\n",
    "    comment_dataset_two.append(info_two)\n",
    "print(\"Comment 0:  \")\n",
    "print(comment_dataset_two[0])\n",
    "\n",
    "with open('r_data/sub_data.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(submission_dataset_two, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open('r_data/com_data.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(comment_dataset_two, file, ensure_ascii=False, indent=4)\n",
    "    \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Tim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data preporcessing part 2\n",
    "# We need to remove anything that has dead data -- like removed, unreconizable words\n",
    "import nltk; nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changes\n",
      "kazooie\n",
      "good\n",
      "evil\n",
      "legend\n",
      "thc98\n",
      "36\n",
      "retro\n",
      "streamers\n",
      "planning\n"
     ]
    }
   ],
   "source": [
    "# Topic model LDA\n",
    "# First attempt is to use Sub title + self text to generate topics\n",
    "# Second attempt is to use just sub title + comments\n",
    "# third attempt is just look at comments\n",
    "# fourth is title + self text + comments\n",
    "#Read in curated submissions\n",
    "# Code by ... from: https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/ for Gensim model\n",
    "# Code by ... from: https://stackabuse.com/python-for-nlp-topic-modeling/ for LDA model\n",
    "\n",
    "# import re\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from pprint import pprint\n",
    "\n",
    "# # Gensim\n",
    "# import gensim\n",
    "# import gensim.corpora as corpora\n",
    "# from gensim.utils import simple_preprocess\n",
    "# from gensim.models import CoherenceModel\n",
    "\n",
    "# # spacy for lemmatization\n",
    "# import spacy\n",
    "\n",
    "# # Plotting tools\n",
    "# import pyLDAvis\n",
    "# import pyLDAvis.gensim  # don't skip this\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# # Enable logging for gensim - optional\n",
    "# import logging\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "# # Handle the NLTK stopwords\n",
    "# from nltk.corpus import stopwords\n",
    "# stop_words = stopwords.words('english')\n",
    "# stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "submission_dataset_three = pd.read_json(r'r_data/sub_data.json')\n",
    "\n",
    "# shows the data\n",
    "# submission_dataset_three.dropna()\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer(max_df=0.8, min_df=2, stop_words='english')\n",
    "doc_term_matrix = count_vect.fit_transform(submission_dataset_three['title'].values.astype('U'))\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "LDA = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "LDA.fit(doc_term_matrix)\n",
    "\n",
    "import random\n",
    "\n",
    "for i in range(10):\n",
    "    random_id = random.randint(0,len(count_vect.get_feature_names()))\n",
    "    print(count_vect.get_feature_names()[random_id])\n",
    "\n",
    "# # open the submissions file\n",
    "# with open('r_data/sub_data.json', 'r', encoding='utf-8') as file:\n",
    "#     submission_dataset_three = json.loads(file.read())\n",
    "\n",
    "\n",
    "#Read in curated comments\n",
    "# with open('r_data/com_data.json', 'r', encoding='utf-8') as file:\n",
    "#     comment_dataset_three = json.loads(file.read())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "need\n",
      "island\n",
      "13\n",
      "super\n",
      "help\n",
      "46\n",
      "mario\n",
      "games\n",
      "speedrun\n",
      "wr\n",
      "Top 10 words for topic #0:\n",
      "['need', 'island', '13', 'super', 'help', '46', 'mario', 'games', 'speedrun', 'wr']\n",
      "\n",
      "\n",
      "Top 10 words for topic #1:\n",
      "['jodenstone', 'weekly', 'thread', 'speed', 'oot', 'time', '18', 'december', 'run', 'wr']\n",
      "\n",
      "\n",
      "Top 10 words for topic #2:\n",
      "['28', 'speed', '10', 'igt', 'souls', 'marathon', 'dark', '12', '50', 'wr']\n",
      "\n",
      "\n",
      "Top 10 words for topic #3:\n",
      "['36', 'race', 'pb', '28', '24', '33', '64', 'rta', '21', 'wr']\n",
      "\n",
      "\n",
      "Top 10 words for topic #4:\n",
      "['hawk', '100', 'mario', 'race', 'super', 'game', 'agdq', 'speedrunning', 'new', 'speedrun']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print words -- manually look at topics\n",
    "first_topic = LDA.components_[0]\n",
    "top_topic_words = first_topic.argsort()[-10:]\n",
    "\n",
    "# for i in top_topic_words:\n",
    "#     print(count_vect.get_feature_names()[i])\n",
    "\n",
    "for i,topic in enumerate(LDA.components_):\n",
    "    print(f'Top 10 words for topic #{i}:')\n",
    "    print([count_vect.get_feature_names()[i] for i in topic.argsort()[-10:]])\n",
    "    print('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(412, 5)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# probability of all topics to each document\n",
    "topic_values = LDA.transform(doc_term_matrix)\n",
    "topic_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2nw69z</td>\n",
       "      <td>[WR]Hotline Miami Any% NG in 20:16 by Dingodrole</td>\n",
       "      <td></td>\n",
       "      <td>37</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2nwjc2</td>\n",
       "      <td>Couple questions about Chrono Trigger Any%</td>\n",
       "      <td>1) Where can I find more information on the rn...</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2nwl8q</td>\n",
       "      <td>[WR] Luigi's Mansion 100% new WR by Veman300! ...</td>\n",
       "      <td></td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2nwwf8</td>\n",
       "      <td>Jak and Daxter any% route?</td>\n",
       "      <td>Does anyone know of a route that is written ou...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2nxll5</td>\n",
       "      <td>[WR] Castlevania:Harmony Of Dissonance Maxim A...</td>\n",
       "      <td></td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                              title  \\\n",
       "0  2nw69z   [WR]Hotline Miami Any% NG in 20:16 by Dingodrole   \n",
       "1  2nwjc2         Couple questions about Chrono Trigger Any%   \n",
       "2  2nwl8q  [WR] Luigi's Mansion 100% new WR by Veman300! ...   \n",
       "3  2nwwf8                         Jak and Daxter any% route?   \n",
       "4  2nxll5  [WR] Castlevania:Harmony Of Dissonance Maxim A...   \n",
       "\n",
       "                                            selftext  score  num_comments  \\\n",
       "0                                                        37             6   \n",
       "1  1) Where can I find more information on the rn...      3             6   \n",
       "2                                                        13             2   \n",
       "3  Does anyone know of a route that is written ou...      1             2   \n",
       "4                                                        16             6   \n",
       "\n",
       "   Topic  \n",
       "0      3  \n",
       "1      4  \n",
       "2      1  \n",
       "3      0  \n",
       "4      3  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see the topic index with max value\n",
    "submission_dataset_three['Topic'] = topic_values.argmax(axis=1)\n",
    "submission_dataset_three.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
